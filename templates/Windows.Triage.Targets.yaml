{{ define "GlobTable" }}
  Target,Rule,Glob,Ref
  {{- range .Rules }}
  {{ .Target}},{{ .Name }},"{{ .Glob }}",{{ .Ref -}}
  {{ end }}
{{ end -}}

name: {{ .Config.Name }}
description: |
  This artifact aims to preserve raw files from the endpoint by
  selectively acquiring the most important files for the investigation
  at hand. The artfiact collates multiple rules from sources such as
  the KapeFiles repository.

  Kape is a popular bulk collector tool for triaging a system
  quickly. While KAPE itself is not an opensource tool, the logic it
  uses to decide which files to collect is encoded in YAML files
  hosted on the KapeFiles project
  (https://github.com/EricZimmerman/KapeFiles) and released under an
  MIT license.

  This artifact is automatically generated from these YAML files,
  contributed and maintained by the community. This artifact only
  encapsulates the KAPE "Targets" - basically a bunch of glob
  expressions used for collecting files on the endpoint. We do not
  do any post processing of these files - we just collect them.

  We recommend that timeouts and upload limits be used
  conservatively with this artifact because we can upload really
  vast quantities of data very quickly.

  NOTE:
    This artifact was built from [The Velociraptor Triage
    Repository](https://triage.velocidex.com/docs/)

  ## DropVerySlowRules

  Some rules are very slow due to a recursive search at the higher
  levels (For example a glob such as `C:\**\*.ini` ). These rules
  cause the collection to be very slow as the entire filesystem must
  be searched.

  By default we drop these rules but you can enable them if you
  like. This will cause the collection to be a lot slower.

  ## CollectionPolicy

  The decision on whether to collect the file or simply display
  metadata (like hash etc) is determined by the CollectionPolicy:

  1. ExcludeSigned: If the file is a signed executable, we do not
     collect it.
  2. HashOnly: Do not collect any file - just display their hashes
  3. AllFiles: Collect all files regardless of their types.

  Commit {{ .Commit }} on {{ .Time }}

reference:
  - https://www.kroll.com/en/insights/publications/cyber/kroll-artifact-parser-extractor-kape
  - https://github.com/EricZimmerman/KapeFiles

parameters:
  - name: HighLevelTargets
    description: A shorter list of Meta-Targets
    type: multichoice
    default: "[]"
    choices:
{{- range .TargetFiles }}
    {{- if hasPrefix "_" .Name }}
    - {{ .Name -}}
    {{ end -}}
{{ end }}

  - name: Targets
    type: multichoice
    description: All targets available
    default: "[]"
    choices:
{{- range .TargetFiles }}
    - {{ .Name -}}
{{ end }}

  - name: Devices
    type: json_array
    description: |
      Name of the drive letter to search. You can add multiple drives
      separated with a comma.
    default: '["C:"]'

  - name: CollectionPolicy
    type: choices
    choices:
      - ExcludeSigned
      - HashOnly
      - AllFiles
    default: ExcludeSigned

  - name: DropVerySlowRules
    type: bool
    default: Y
    description: Ignore inefficient targets

  - name: TrustedPathRegex
    type: regex
    default: ^C:\\\\Windows\\\\
    description: |
      Do not hash or upload any adaptive files matching this regex.

  - name: MaxFileSize
    type: int
    description: |
      The max size in bytes of the individual files to collect
      (Default 100Mb).

  - name: MaxHashSize
    type: int
    default: "100000000"
    description: |
      The max size in bytes of the individual files to hash.

  - name: UPLOAD_IS_RESUMABLE
    type: bool
    default: Y
    description: |
      If set the uploads can be resumed if the flow times out or
      errors.

  - name: VSS_MAX_AGE_DAYS
    type: int
    default: 0
    description: |
      If larger than zero we analyze VSS within this many days
      ago. (e.g 7 will analyze all VSS within the last week).  Note
      that when using VSS analysis we have to use the ntfs accessor
      for everything which will be much slower.

  - name: Verbose
    type: bool
    description: |
      Add Verbose debugging logs

  - name: DISABLE_DANGEROUS_API_CALLS
    type: bool
    default: "Y"

  - name: WORKERS
    type: int
    default: "5"
    description: |
      Number of concurrent workers to use for parsing and compressing.

export: |
  -- We need to materialize into a scope variable. If this is too
  -- large, VQL will use too much memory keeping these objects alive.
  LET VQL_MATERIALIZE_ROW_LIMIT <= 10
  LET NTFS_CACHE_TIME <= 100000
  LET NTFS_DISABLE_FULL_PATH_RESOLUTION <= TRUE

  // Initialize libmagic before we call it from multiple threads.
  LET _ <= magic(path="", accessor="data")

  LET S = scope()
  LET Verbose <= S.Verbose || FALSE
  LET MaybeLOG(Message, Args) = if(condition=Verbose,
     then=log(message=Message, level="DEBUG", dedup= -1, args=Args),
     else=TRUE)

  // Only enable resuming if the upload() plugin version is recent
  // enough. Earlier versions could lead to crashes in some cases.
  LET UPLOAD_IS_RESUMABLE <= S.UPLOAD_IS_RESUMABLE && version(
     function="upload") > 2

  LET CollectionPolicy <= S.CollectionPolicy || "ExcludeSigned"

  // Helper for VQL targets - try to download the file, but if failed,
  // we return an empty row to record the filename.
  LET TryToDownload(OSPath, Row) = SELECT *,
     Data + dict(Details=GetDetails(OSPath=OSPath) || dict()) AS Data
  FROM switch(
  a={
    SELECT *, "auto" AS Accessor, Row AS Data
    FROM stat(filename=OSPath, accessor="auto")
  }, b={
    SELECT OSPath,
           0 AS Size,
           NULL AS Btime,
           NULL AS Ctime,
           NULL AS Mtime,
           NULL AS Atime,
           "" AS Accessor,
           Row AS Data
    FROM scope()
  })

  {{- if .Config.Debug }}
  LET GlobTable <= '''{{ template "GlobTable" . }}
  '''
  {{ else }}
  LET GlobTable <= gunzip(string=base64decode(string="{{ Compress "GlobTable" . }}"))
  {{ end }}

  LET SlowGlobRegex <= if(condition=S.DropVerySlowRules,
     then="^\\*\\*", else="RunSlowFileGlobs!!!")

  -- Group the targets for faster searching.
  LET TargetTable <= SELECT Target,
       enumerate(items=dict(Rule=Rule, Glob=Glob, Ref=Ref)) AS Rules
    FROM parse_csv(accessor="data",
  filename=GlobTable)
  GROUP BY Target

  //  Build a lookup cache on target.
  LET Lookup <= memoize(query={
    SELECT * FROM TargetTable
  }, key="Target")

  -- Extract all rules within the required target. Uses the memoized
  -- structure above.
  LET FilterTable(Required) =
     SELECT Required AS Target, *
     FROM flatten(query={
       SELECT * FROM foreach(row=get(item=Lookup, field=Required).Rules)
     })
     WHERE if(condition=Glob =~ SlowGlobRegex,
              then=log(
                level="INFO",
                message="Dropping rule %v/%v because it is too slow: %v",
                dedup=-1,
                args=[Target, Rule, Glob]) AND FALSE,
              else=TRUE)

  LET Expand(FilteredTable) = SELECT * FROM foreach(
  row=FilteredTable,
  query={
    -- If there is a reference, resolve it from the table recursively.
    SELECT *
    FROM if(condition=Ref AND MaybeLOG(
       Message="%v/%v: Resolving Ref %v",
       Args=[Target, Rule, Ref]),
    then={
       SELECT * FROM Expand(
          FilteredTable={
             SELECT * FROM FilterTable(Required=Ref)
          })
    }, else={
       SELECT Target, Rule, Glob FROM scope()
       WHERE (Glob && MaybeLOG(
         Message="%v/%v: Glob is %v",
         Args=[Target, Rule, Glob])) || TRUE
    })
  })

  -- Collect all the top level targets that the user selected.
  LET Collections(Targets) = SELECT Target + "/" + Rule AS Rule, Glob
    FROM Expand(FilteredTable={
      SELECT Target,
            Rules.Rule AS Rule,
            Rules.Glob AS Glob,
            Rules.Ref AS Ref
     FROM flatten(query={
       SELECT * FROM TargetTable
       WHERE get(item=Targets, field=Target)
       AND MaybeLOG(
         Message="Collecting target %v, Rules: %v",
         Args=[Target, Rules.Rule])
     })
    })
    GROUP BY Rule, Glob

    // In ExcludeSigned and HashOnly we dont upload signed binaries.
    LET ShouldUploadSignedBinary <= dict(
       ShouldUpload = NOT CollectionPolicy =~ "ExcludeSigned|HashOnly")

    // In HashOnly mode we never upload anything.
    LET ShouldUploadAnyFile <= dict(
       ShouldUpload = NOT CollectionPolicy =~ "HashOnly")

    LET DoNotUpload <= dict(ShouldUpload=FALSE)

    // Determine if we should upload the file based on signature.
    LET ShouldUpload(Details) = if(
      condition=OSPath =~ TrustedPathRegex AND
                MaxFileSize > 0 AND Details.Stat.Size > MaxFileSize,
      then= Details + DoNotUpload,
      else=if(
       // What to do about binaries? If they have an issuer name then
       // they are signed.
       condition=Details.Signatures.IssuerName,
       then=Details + ShouldUploadSignedBinary,
       else=Details + ShouldUploadAnyFile))

    // If the file is a binary, also add authenticode information.
    LET MaybeBinary(OSPath, Details) = ShouldUpload(Details=if(
       condition=Details.Magic =~ "PE.+executable",
       then=Details + dict(Signatures=authenticode(filename=OSPath)),
       else=Details))


    // Hash the file if it is not too large
    LET MaybeHash(OSPath, Details) = if(
      condition=NOT OSPath =~ TrustedPathRegex AND
            Details.Stat AND Details.Stat.Size < MaxHashSize,
      then=Details + dict(Hashes=hash(path=OSPath),
                          Magic=magic(path=OSPath)),
      else=Details)

    // Calculate the details column with hashes and magic.
    LET _GetDetails(OSPath) = MaybeBinary(
       OSPath=OSPath,
       Details=MaybeHash(
         OSPath=OSPath,
         Details=dict(filename=OSPath,
                      Stat=OSPath && stat(filename=OSPath))))

    // Cache the hashing for speedup.
    LET GetDetails(OSPath) = cache(
       period=100000,
       func= _GetDetails(OSPath=OSPath),
       name="GetDetails", key=OSPath.String)

    LET _ExpandedTransforms <= dict(
        `^\\\\SystemRoot\\\\`="%SystemRoot%\\",
        `^system32\\\\`="%SystemRoot%\\System32\\",
        `^{.+}.+`="\\$0",
        `^.:\\\\Program Files[^\\\\]*\\\\[^ ]+`='"$0"',
        `^%[^ ]+%[^ ]+`='"$0"'
      )

    // Extract the binary from the command line
    LET ExpandPath(Path) = lowcase(string=commandline_split(
    command=expand(path=regex_transform(source=Path,
      map=_ExpandedTransforms)))[0])

    {{ range $idx, $r := .Rules -}}
    {{- if $r.VQL -}}
    // {{ .Description }}
    LET Collect{{ $r.Target }}_{{ $r.Name }} =
{{ Indent .VQL 7 -}}
    {{- end -}}
    {{- end }}

    {{- range .TargetFiles -}}
    {{- if .Preamble }}
    // From {{ .Name }}
{{ Indent .Preamble 4 -}}
    {{- end -}}
    {{- end }}

sources:
- name: SearchGlobs
  notebook:
    - type: none
  query: |
{{ if .Dependencies }}
    // Work around bug in the artifact compiler failing to detect deps in export section
{{ range $k, $v := .Dependencies }}
    LET _ = SELECT * FROM Artifact.{{ $k }}()
{{ end }}
{{ end }}
    LET Targets <= to_dict(item={
      SELECT _value AS _key, TRUE AS _value
      FROM foreach(row=Targets + HighLevelTargets)
    })

    LET AllCollections <= Collections(Targets=Targets)

    SELECT * FROM AllCollections
    --WHERE Glob

- name: All Matches Metadata
  notebook:
    - type: vql
      template: |
       // This cell generates other cells to preview the collected
       // data.  DO NOT recalculate this cell - each time new cells
       // will be added. Instead delete the notebook and allow
       // Velociraptor to recreate the entire notebook.
       LET ArtifactsWithResults <=
         SELECT pathspec(accessor="fs", parse=Data.VFSPath)[4] AS Artifact ,
           pathspec(accessor="fs", parse=Data.VFSPath)[-1][:-5] AS Source ,
           stat(accessor="fs", filename=Data.VFSPath + ".index").Size / 8 AS Records
         FROM enumerate_flow(client_id=ClientId, flow_id=FlowId)
         WHERE Type =~ "Result" AND Records > 0

       LET _ <= SELECT notebook_update_cell(notebook_id=NotebookId, type="vql",
       input=format(format='''
       /*
       # Results From %v
       */
       SELECT * FROM source(source=%q)
       ''', args=[Source, Source]),
       output=format(format='''
       <i>Recalculate</i> to show Results from <b>%v</b> with <b>%v</b> rows
       ''', args=[Source, Records])) AS NotebookModification
       FROM ArtifactsWithResults

       /*
       # Results Overview
       */
       SELECT Source, Records FROM ArtifactsWithResults ORDER BY Source


  query: |
    LET GlobLookup <= memoize(query=AllCollections, key="Glob")
    LET NTFSGlobs = SELECT * FROM AllCollections
       WHERE Glob AND Glob =~ "[:$]" AND NOT Glob =~ "\\$Recycle.Bin"
    LET AutoGlobs = SELECT * FROM AllCollections
       WHERE Glob AND ( Glob =~ "\\$Recycle.Bin" OR NOT Glob =~ "[:$]" )

    LET _ <= MaxFileSize > 0 && MaybeLOG(
      Message="Limiting file acquisition to MaxFileSize %v bytes (%v)",
      Args=[MaxFileSize, humanize(bytes=MaxFileSize)])

    LET PreferredAccessor <= if(
       condition=VSS_MAX_AGE_DAYS > 0,
       then="ntfs_vss", else="auto")

    LET AllGlobs = SELECT *
    FROM foreach(row={
        SELECT _value AS Device FROM foreach(row=Devices)
    }, query={
      SELECT * FROM chain(
      GlobNTFS={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               "ntfs" AS Accessor,
               dict(Globs=Globs) AS Data,
               "Glob" AS Type
        FROM glob(globs=NTFSGlobs.Glob, accessor="ntfs", root=Device)
      },
      GlobAuto={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               PreferredAccessor AS Accessor,
               dict(Globs=Globs) AS Data,
               "Glob" AS Type
        FROM glob(globs=AutoGlobs.Glob,
                  accessor=PreferredAccessor,
                  root=Device)
      })
    })
    WHERE NOT IsDir

    LET AllResults <= SELECT Type,
                             OSPath AS SourceFile,
                             Size,
                             Btime AS Created,
                             Ctime AS Changed,
                             Mtime AS Modified,
                             Atime AS LastAccessed,
                             Accessor,
                             Data
    FROM chain(async=WORKERS > 0,
      {{ range $idx, $r := .Rules -}}
      {{- if $r.VQL -}}
      {{ $r.Target }}_{{ $r.Name }}={
        SELECT * FROM if(condition={
          SELECT * FROM AllCollections
          WHERE Rule = "{{ $r.Target }}/{{ $r.Name }}"
            AND MaybeLOG(
              Message="Collecting Custom VQL Rule %s",
              Args="{{ $r.Target }}/{{ $r.Name }}")
        }, then={
          SELECT "{{ $r.Target }}/{{ $r.Name }}" AS Type,
                 "{{ $r.Target }}/{{ $r.Name }}" AS Rule,
                 *
          FROM Collect{{ $r.Target }}_{{ $r.Name }}
        })
      },
      {{- end -}}
      {{- end }}
      Globs=AllGlobs
    )
    WHERE log(level="INFO", message="Found %v for rule %v", args=[
              SourceFile, Rule], dedup=10)

    SELECT * FROM AllResults
{{ range $idx, $r := .Rules -}}
{{- if $r.VQL }}
- name: {{ $r.Target }}_{{ $r.Name }}
  notebook:
    - type: none
  query: |
    SELECT * FROM foreach(row={
      SELECT dict(SourceFile=SourceFile, Size=Size, Modified=Modified) + Data AS Data
      FROM AllResults
      WHERE Type = "{{ $r.Target }}/{{ $r.Name }}"
    }, column="Data")
{{- end -}}
{{- end }}

- name: Uploads
  query: |
    -- Upload the files. Split into workers so the files are uploaded
    -- in parallel.
    LET uploaded_files = SELECT *
    FROM foreach(row={
       SELECT *
       FROM AllResults
       WHERE Size > 0
       GROUP BY SourceFile
       },
          workers=WORKERS,

          // Do the heavy lifting in a thread
          query={
            SELECT * FROM foreach(row={
              SELECT GetDetails(OSPath=SourceFile) AS Details
              FROM scope()
            }, query={
              SELECT timestamp(epoch=now()) AS CopiedOnTimestamp,
                     Created,
                     Changed,
                     LastAccessed,
                     Modified,
                     SourceFile,
                     Size,
                     Details,
                     if(condition=Details.ShouldUpload,
                        then=upload(file=SourceFile,
                                    accessor=Accessor,
                                    mtime=Modified)) AS Upload
              FROM scope()
          })
      })

    -- Separate the hashes into their own column.
    SELECT CopiedOnTimestamp,
           SourceFile,
           Upload.Path AS DestinationFile,
           Size AS FileSize,
           Details.Hash.SHA256 AS SourceFileSha256,
           Created,
           Changed,
           Modified,
           LastAccessed,
           Details,
           Upload
    FROM uploaded_files

  notebook:
    - type: none
    - type: vql_suggestion
      name: Post process collection
      template: |
        /*

        # Post process this collection.

        Uncomment the following and evaluate the cell to create new
        collections based on the files collected from this artifact.

        The below VQL will apply remapping so standard artifacts will
        see the KapeFiles.Targets collection below as a virtual
        Windows Client. The artifacts will be collected to a temporary
        container and then re-imported as new collections into this
        client.

        NOTE: This is only a stop gap in case the proper artifacts
        were not collected in the first place. Parsing artifacts
        through a remapped collection is not as accurate as parsing
        directly on the endpoint. See
        https://docs.velociraptor.app/training/playbooks/preservation/
        for more info.

        */
        LET _ <= import(artifact="Windows.KapeFiles.Remapping")

        LET tmp <= tempfile()

        LET Results = SELECT import_collection(filename=Container, client_id=ClientId) AS Import
        FROM collect(artifacts=[
                       "Windows.Forensics.Usn",
                       "Windows.NTFS.MFT",
                     ],
                     args=dict(`Windows.Forensics.Usn`=dict(),
                               `Windows.NTFS.MFT`=dict()),
                     output=tmp,
                     remapping=GetRemapping(FlowId=FlowId, ClientId=ClientId))

        // SELECT * FROM Results

column_types:
- name: CopiedOnTimestamp
  type: timestamp
- name: Data
  type: json/1
- name: Details
  type: json/1
